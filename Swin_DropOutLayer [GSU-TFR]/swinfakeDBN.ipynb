{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":939937,"sourceType":"datasetVersion","datasetId":501529}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":28694.233414,"end_time":"2024-04-07T13:15:02.479450","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-07T05:16:48.246036","version":"2.5.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"eeb8674c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport seaborn as sns\nimport torch\nfrom torchsummary import summary\nfrom torchvision import models\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import Subset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torch.autograd import Variable\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom tqdm import tqdm\nimport timm  # For loading Swin Transformer models\nimport lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport shutil\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ncode_dir = '/kaggle/working'\nmodel_dir = '/kaggle/working'\noutput_dir = '/kaggle/working/output'\n\nif not os.path.exists(code_dir):\n    os.mkdir(code_dir)\n\nif not os.path.exists(model_dir):\n    os.mkdir(model_dir)\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n","metadata":{"id":"eeb8674c","papermill":{"duration":17.50985,"end_time":"2024-04-07T05:17:09.390140","exception":false,"start_time":"2024-04-07T05:16:51.880290","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:22:37.547968Z","iopub.execute_input":"2025-05-13T13:22:37.548231Z","iopub.status.idle":"2025-05-13T13:22:42.624034Z","shell.execute_reply.started":"2025-05-13T13:22:37.548209Z","shell.execute_reply":"2025-05-13T13:22:42.623465Z"}},"outputs":[],"execution_count":1},{"id":"9d9dc4cf","cell_type":"code","source":"# Set seed for reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:22:42.625001Z","iopub.execute_input":"2025-05-13T13:22:42.625432Z","iopub.status.idle":"2025-05-13T13:22:42.658817Z","shell.execute_reply.started":"2025-05-13T13:22:42.625404Z","shell.execute_reply":"2025-05-13T13:22:42.658195Z"}},"outputs":[],"execution_count":2},{"id":"c9e67682","cell_type":"code","source":"# Configuration\nclass Config:\n    data_dir = \"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake\"  # Update this path\n    batch_size = 32\n    num_workers = 4\n    image_size = 224  # Swin Transformer generally uses 224x224\n    num_epochs = 10\n    learning_rate = 1e-4\n    weight_decay = 1e-4\n    model_name = \"swin_base_patch4_window7_224\"  # Smaller version for faster training\n    checkpoint_path = os.path.join(model_dir, \"swin_dropNull.pth\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Learning rate scheduler settings\n    scheduler_type = \"cosine\"  # Options: \"cosine\", \"plateau\", None\n    min_lr = 1e-6  # Minimum learning rate for cosine scheduler\n    patience = 3   # Patience for ReduceLROnPlateau\n    cooldown = 1   # Cooldown period after reducing LR\n    lr_factor = 0.1  # Factor by which to reduce LR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:25.753822Z","iopub.execute_input":"2025-05-13T13:23:25.754587Z","iopub.status.idle":"2025-05-13T13:23:25.760755Z","shell.execute_reply.started":"2025-05-13T13:23:25.754560Z","shell.execute_reply":"2025-05-13T13:23:25.760029Z"}},"outputs":[],"execution_count":3},{"id":"3d376594","cell_type":"code","source":"def train_data_transform(model_type, data_fraction, batch_size, shuffle=True, num_workers=4, pin_memory=True):\n    if model_type == \"swin\":\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n    full_train_dataset = ImageFolder(\"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/train\", transform=transform)\n    num_train_data = int(len(full_train_dataset) * data_fraction)\n    train_indices = random.sample(range(len(full_train_dataset)), num_train_data)\n    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=Config.num_workers, pin_memory=True)\n\n    return train_dataloader\n","metadata":{"id":"3d376594","papermill":{"duration":0.025548,"end_time":"2024-04-07T05:17:10.364693","exception":false,"start_time":"2024-04-07T05:17:10.339145","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:27.591348Z","iopub.execute_input":"2025-05-13T13:23:27.592040Z","iopub.status.idle":"2025-05-13T13:23:27.597612Z","shell.execute_reply.started":"2025-05-13T13:23:27.592017Z","shell.execute_reply":"2025-05-13T13:23:27.596937Z"}},"outputs":[],"execution_count":4},{"id":"ac574ea5","cell_type":"code","source":"def val_data_transform(model_type, data_fraction, batch_size, shuffle=False, num_workers=4, pin_memory=True):\n    # Common ImageNet normalization mean and std\n   \n    if model_type == 'swin':\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        # Manually define the transform for Swin using the same ImageNet normalization\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n    \n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n\n    full_valid_dataset = ImageFolder(\"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/valid\", transform=transform)\n\n    num_valid_data = int(len(full_valid_dataset) * data_fraction)\n    random.seed(42)\n    valid_indices = random.sample(range(len(full_valid_dataset)), num_valid_data)\n    valid_dataset = torch.utils.data.Subset(full_valid_dataset, valid_indices)\n\n    val_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n    return val_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:27.932602Z","iopub.execute_input":"2025-05-13T13:23:27.933451Z","iopub.status.idle":"2025-05-13T13:23:27.939220Z","shell.execute_reply.started":"2025-05-13T13:23:27.933416Z","shell.execute_reply":"2025-05-13T13:23:27.938434Z"}},"outputs":[],"execution_count":5},{"id":"5887ebe5","cell_type":"code","source":"def test_data_transform(model_type, data_fraction, batch_size, shuffle=False, num_workers=4, pin_memory=True):\n    # Common ImageNet normalization mean and std     \n    if model_type == 'swin':\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        # Manually define the transform for Swin using the same ImageNet normalization\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n    \n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n    \n    full_test_dataset = ImageFolder(\"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test\", transform=transform)\n\n    num_test_data = int(len(full_test_dataset) * data_fraction)\n    random.seed(42)\n    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n\n    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return test_dataloader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:28.177462Z","iopub.execute_input":"2025-05-13T13:23:28.177734Z","iopub.status.idle":"2025-05-13T13:23:28.183402Z","shell.execute_reply.started":"2025-05-13T13:23:28.177716Z","shell.execute_reply":"2025-05-13T13:23:28.182752Z"}},"outputs":[],"execution_count":6},{"id":"4bf0c7f5","cell_type":"markdown","source":"# Orginal training process","metadata":{"id":"4bf0c7f5","papermill":{"duration":0.012731,"end_time":"2024-04-07T05:17:10.575599","exception":false,"start_time":"2024-04-07T05:17:10.562868","status":"completed"},"tags":[]}},{"id":"2a94dd17","cell_type":"code","source":"class AttentionPool(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Conv2d(in_features, 1, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        # Make sure x is in [B, C, H, W] format\n        if x.dim() == 4 and x.shape[1] != self.attention[0].in_channels:\n            # If channels are in the wrong position (likely [B, H, W, C])\n            x = x.permute(0, 3, 1, 2)  # Change to [B, C, H, W]\n            \n        weights = self.attention(x)\n        weighted_x = x * weights\n        return weighted_x.sum(dim=(2, 3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:28.618461Z","iopub.execute_input":"2025-05-13T13:23:28.618709Z","iopub.status.idle":"2025-05-13T13:23:28.623348Z","shell.execute_reply.started":"2025-05-13T13:23:28.618692Z","shell.execute_reply":"2025-05-13T13:23:28.622599Z"}},"outputs":[],"execution_count":7},{"id":"f0e14cab","cell_type":"code","source":"import math\n\nclass SwinFaceDetector(nn.Module):\n    def __init__(self, model_name=Config.model_name, num_classes=2, \n                 dropout_rate=0.1, hidden_dim=512, pretrained=True):\n        super(SwinFaceDetector, self).__init__()\n        \n        # Load backbone with no global pooling\n        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='')\n        \n        # Get feature dimension\n        self.feature_dim = self.backbone.num_features\n        \n        # Add attention pooling layer\n        self.global_pool = AttentionPool(self.feature_dim)\n        \n        # Define classifier with configurable architecture\n        self.classifier = nn.Sequential(\n            nn.Linear(self.feature_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, num_classes)\n        )\n        \n        # Initialize for Grad-CAM\n        self.activations = None\n        self.gradients = None\n        \n        # Register hook for Grad-CAM\n        self._register_hooks()\n        \n        # Print model summary\n        #print(f\"✅ Initialized {model_name} with {self.feature_dim} features\")\n        #print(f\"✅ Classifier: {hidden_dim} hidden units, {dropout_rate} dropout\")\n        \n    def _register_hooks(self):\n        \"\"\"Register hooks for Grad-CAM\"\"\"\n        if hasattr(self.backbone, 'layers') and hasattr(self.backbone.layers[-1], 'blocks'):\n            # For Swin Transformer architectures\n            target_layer = self.backbone.layers[-1].blocks[-1]\n            target_layer.register_forward_hook(self._save_activation)\n        else:\n            # Fallback for other architectures - find a suitable layer\n            print(\"⚠️ Backbone doesn't have the expected layer structure.\")\n            print(\"⚠️ Grad-CAM may not work correctly.\")\n            # You might need to find an alternative target layer based on your backbone\n\n    def _save_activation(self, module, input, output):\n        \"\"\"Save activations for Grad-CAM\"\"\"\n        self.activations = output\n\n        # Only register gradient hook if we're in training mode or gradients are enabled\n        if output.requires_grad:\n            output.register_hook(self._save_gradient)\n        \n    def _save_gradient(self, grad):\n        \"\"\"Save gradients for Grad-CAM\"\"\"\n        self.gradients = grad\n        \n    def forward(self, x):\n        \"\"\"Forward pass through the network\"\"\"\n        # Get input shape for debugging\n        batch_size, channels, height, width = x.shape\n        \n        # Extract features from backbone\n        features = self.backbone.forward_features(x)\n        \n        # Print feature shape to debug\n        #print(f\"Debug - Feature shape from backbone: {features.shape}\")\n        \n        # Handle different feature formats depending on backbone architecture\n        if len(features.shape) == 3:  # B×L×C format (transformer output)\n            # This is likely from a ViT or Swin transformer\n            batch_size, seq_len, channels = features.shape\n            \n            # For Swin models, features might need reshaping to spatial form\n            # Check if sequence length can be a perfect square for spatial interpretation\n            h = int(math.sqrt(seq_len))\n            if h * h == seq_len:\n                # Reshape to [B, C, H, W]\n                features = features.permute(0, 2, 1).reshape(batch_size, channels, h, h)\n            else:\n                # If not a perfect square, just do global average pooling\n                features = features.mean(dim=1)\n                return self.classifier(features)  # Skip attention pooling\n        elif len(features.shape) == 4 and features.shape[1] > features.shape[2]:\n            # Already in [B, C, H, W] format\n            pass\n        elif len(features.shape) == 4 and features.shape[3] > features.shape[1]:\n            # In [B, H, W, C] format, which needs permutation\n            features = features.permute(0, 3, 1, 2)\n        \n        # Apply attention pooling\n        #print(f\"Debug - Feature shape before attention pooling: {features.shape}\")\n        pooled_features = self.global_pool(features)\n        #print(f\"Debug - Feature shape after pooling: {pooled_features.shape}\")\n        \n        # Pass through classifier\n        logits = self.classifier(pooled_features)\n        \n        return logits\n        \n    def freeze_backbone(self):\n        \"\"\"Freeze all backbone parameters\"\"\"\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n            \n    def unfreeze_stages(self, num_stages=1):\n        \"\"\"Unfreeze the last n stages of the backbone\"\"\"\n        # First freeze everything\n        self.freeze_backbone()\n        \n        # Check if the backbone has the expected structure\n        if hasattr(self.backbone, 'layers'):\n            # Then unfreeze the specified number of final stages\n            stages_to_unfreeze = self.backbone.layers[-num_stages:]\n            for stage in stages_to_unfreeze:\n                for param in stage.parameters():\n                    param.requires_grad = True\n                    \n            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in self.parameters())\n            print(f\"✅ Unfroze {num_stages} stage(s). Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params:.2%})\")\n        else:\n            print(\"⚠️ Backbone doesn't have the expected layer structure for unfreezing stages.\")\n        \n    def unfreeze_layers(self, num_layers_to_unfreeze):\n        \"\"\"Unfreeze the last n blocks of the final stage\"\"\"\n        # Freeze all blocks first\n        self.freeze_backbone()\n        \n        # Check if the backbone has the expected structure\n        if hasattr(self.backbone, 'layers') and hasattr(self.backbone.layers[-1], 'blocks'):\n            # Unfreeze last N blocks from the final stage\n            last_stage_blocks = self.backbone.layers[-1].blocks\n            total_blocks = len(last_stage_blocks)\n            start = max(0, total_blocks - num_layers_to_unfreeze)\n            \n            for block in last_stage_blocks[start:]:\n                for param in block.parameters():\n                    param.requires_grad = True\n            \n            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in self.parameters())\n            print(f\"✅ Unfroze {num_layers_to_unfreeze} block(s) from the last stage. Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params:.2%})\")\n        else:\n            print(\"⚠️ Backbone doesn't have the expected layer structure for unfreezing layers.\")\n    \n    # Grad-CAM methods\n    def get_cam_weights(self):\n        \"\"\"Get class activation map weights from the gradients\"\"\"\n        if self.gradients is None:\n            print(\"⚠️ No gradients available. Did you run backward() after the forward pass?\")\n            return None\n        \n        # Handle different gradient formats\n        if len(self.gradients.shape) == 4:  # For CNNs: [B, C, H, W]\n            weights = torch.mean(self.gradients, dim=[0, 2, 3]).detach()\n        elif len(self.gradients.shape) == 3:  # For transformers: [B, L, C]\n            weights = torch.mean(self.gradients, dim=[0, 1]).detach()\n        else:\n            weights = self.gradients.mean(0).detach()\n            \n        return weights\n    \n    def get_activations(self):\n        return self.activations\n    \n    def get_activations_gradient(self):\n        return self.gradients\n        \n    def generate_cam(self, target_class=None):\n        \"\"\"Generate a class activation map for the target class\"\"\"\n        # Get weights from gradients\n        weights = self.get_cam_weights()\n        if weights is None:\n            return None\n            \n        # Get activations\n        activations = self.get_activations().detach()\n        \n        # Handle different activation formats\n        if len(activations.shape) == 4:  # For CNNs: [B, C, H, W]\n            # Create weighted sum of activations\n            cam = torch.zeros(activations.shape[2:], dtype=torch.float32, device=activations.device)\n            \n            # Use target_class weights or average all class weights\n            if target_class is not None:\n                cam = torch.sum(weights[target_class] * activations[0], dim=0)\n            else:\n                cam = torch.sum(weights.view(-1, 1, 1) * activations[0], dim=0)\n        elif len(activations.shape) == 3:  # For transformers: [B, L, C]\n            # For transformers, reshape to spatial dimensions if possible\n            seq_len = activations.shape[1]\n            h = int(math.sqrt(seq_len))\n            \n            if h * h == seq_len:  # Perfect square\n                # Reshape to spatial form\n                if target_class is not None:\n                    cam = torch.sum(weights[target_class] * activations[0], dim=1)\n                else:\n                    cam = torch.sum(weights.view(-1, 1) * activations[0], dim=1)\n                cam = cam.reshape(h, h)\n            else:\n                # Can't reshape to square, just use as is\n                if target_class is not None:\n                    cam = torch.sum(weights[target_class] * activations[0], dim=1)\n                else:\n                    cam = torch.sum(weights.view(-1, 1) * activations[0], dim=1)\n        else:\n            print(f\"⚠️ Unexpected activation shape: {activations.shape}\")\n            return None\n            \n        # ReLU on the CAM to show only positive influences\n        cam = torch.maximum(cam, torch.tensor(0.))\n        \n        # Normalize between 0-1\n        if torch.max(cam) > 0:\n            cam = cam / torch.max(cam)\n            \n        return cam.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:28.837455Z","iopub.execute_input":"2025-05-13T13:23:28.838172Z","iopub.status.idle":"2025-05-13T13:23:28.858295Z","shell.execute_reply.started":"2025-05-13T13:23:28.838150Z","shell.execute_reply":"2025-05-13T13:23:28.857410Z"}},"outputs":[],"execution_count":8},{"id":"b0844113","cell_type":"code","source":"# Training function\ndef train(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n    best_f1 = 0.0\n    all_metrics = []  # Store metrics per epoch\n    trigger_times = 0\n    patience = 3;\n    unfreeze_interval = 1\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print('-' * 10)\n\n        # Gradual unfreezing\n        layers_to_unfreeze = min((epoch // unfreeze_interval) + 1, 4)  # Unfreeze up to 4 blocks\n        model.unfreeze_layers(layers_to_unfreeze)\n        print(f\"Unfreezing the last {layers_to_unfreeze} blocks\")\n\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0.0\n        y_true_train, y_pred_train = [], []\n        train_loss, train_accuracy, train_f1 = 0.0, 0.0, 0.0\n        val_loss, val_accuracy, val_f1 = 0.0, 0.0, 0.0\n        learning_rates = 0.0\n\n        # Store current learning rate\n        current_lr = optimizer.param_groups[0]['lr']\n        all_metrics.append(current_lr)\n        print(\"=====Lr=====\")\n        print(device)\n        print(f\"Current learning rate: {current_lr:.1e}\")\n\n        loop = tqdm(enumerate(train_loader), total = len(train_loader), desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n        \n        for bathc_idx, (inputs, labels) in loop:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            \n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n            #_, preds = torch.max(outputs, 1) works same as this but in cleaner way\n            predicted = outputs.argmax(dim=1)\n            y_true_train.extend(labels.cpu().numpy())\n            y_pred_train.extend(predicted.cpu().numpy())\n            loop.set_postfix(loss=loss.item())\n        \n        train_loss = running_loss / len(train_loader.dataset)\n        train_accuracy = accuracy_score(y_true_train, y_pred_train)\n        train_f1 = f1_score(y_true_train, y_pred_train, average='weighted')\n    \n        print(f\"Train Loss: {train_loss:.6f} | Acc: {train_accuracy*100:.2f}% | F1: {train_f1:.6f}\")\n        \n        # Validation phase\n        model.eval()\n        val_running_loss = 0.0\n        running_corrects = 0\n        y_true_val, y_pred_val = [], []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n                \n                # Statistics\n                val_running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                predicted = outputs.argmax(dim=1)\n                y_true_val.extend(labels.cpu().numpy())\n                y_pred_val.extend(predicted.cpu().numpy())\n        \n        val_loss = running_loss / len(val_loader.dataset)\n        val_accuracy = accuracy_score(y_true_val, y_pred_val)\n        _, _, val_f1, _ = precision_recall_fscore_support(y_true_val, y_pred_val, average='macro', zero_division=0)\n        \n        print(f\"Val Loss: {val_loss:.6f} | Acc: {val_accuracy*100:.2f}% | F1: {val_f1:.6f}\")\n        \n        # Learning rate scheduler step\n        if scheduler is not None:\n            # Step for ReduceLROnPlateau depends on val_loss\n            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                scheduler.step(val_loss)\n            else:\n                scheduler.step()\n        \n        # Save best model\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            trigger_times = 0\n            torch.save(model.state_dict(), Config.checkpoint_path)\n            #print(f\"Saved best model with accuracy: {best_acc:.4f}\")\n            print(f\"✅ Model saved at {Config.checkpoint_path}\")\n        else:\n            trigger_times += 1\n            print(f\"❌ No improvement in validation f1. Patience: {trigger_times}/{patience}\")\n            if trigger_times >= patience:\n                print(\"⛔ Early stopping triggered.\")\n                break\n                \n        # Store metrics per epoch\n        all_metrics.append({\n            'epoch': epoch + 1,\n            'train_loss': train_loss,\n            'train_accuracy': train_accuracy,\n            'train_f1': train_f1,\n            'val_loss': val_loss,\n            'val_accuracy': val_accuracy,\n            'val_f1': val_f1\n        })\n\n    # Save metrics to CSV after the last epoch\n    df = pd.DataFrame(all_metrics)\n    df.to_csv('metrics_Swin.csv', index=False)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:29.068639Z","iopub.execute_input":"2025-05-13T13:23:29.069226Z","iopub.status.idle":"2025-05-13T13:23:29.081935Z","shell.execute_reply.started":"2025-05-13T13:23:29.069202Z","shell.execute_reply":"2025-05-13T13:23:29.081136Z"}},"outputs":[],"execution_count":9},{"id":"86c15165","cell_type":"code","source":"# Evaluation function\ndef evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    y_pred = []\n    y_true = []\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            predicted = outputs.argmax(dim=1)\n        \n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n    \n    test_loss = running_loss / len(list(dataloader.dataset))\n    test_accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, test_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n\n    print(f\"Test Loss: {test_loss:.6f} | Test Accuracy: {(test_accuracy * 100):.2f}% | Test F1-Score: {test_f1:.6f}\")\n    \n    # Compute metrics\n    cm = confusion_matrix(y_true, y_pred)\n    report = classification_report(y_true, y_pred, target_names=[\"fake\", \"real\"])\n    print(report)\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"fake\", \"real\"], yticklabels=[\"fake\", \"real\"])\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('FaceSwin Confusion Matrix')\n    plt.savefig('confusion_SWINmatrix.png')\n    \n    return cm, report, y_true, y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:36.827372Z","iopub.execute_input":"2025-05-13T13:23:36.827675Z","iopub.status.idle":"2025-05-13T13:23:36.836169Z","shell.execute_reply.started":"2025-05-13T13:23:36.827655Z","shell.execute_reply":"2025-05-13T13:23:36.835298Z"}},"outputs":[],"execution_count":10},{"id":"52d2da0b","cell_type":"code","source":"def main(data_fraction=1, model_type='swin', batch_size=Config.batch_size):\n   \n    print(f\"Using device: {Config.device}\")\n    df = pd.DataFrame()\n     # Dataset info for logging\n    num_train_data = int(100000 * data_fraction)\n    num_valid_data = int(20000 * data_fraction)\n    print(f\"Training samples: {num_train_data}\")\n    print(f\"Validation samples: {num_valid_data}\")\n    print(\"====================Starting==================================\")\n    \n     # Load data\n    train_dataloader = train_data_transform(\n        model_type, data_fraction, batch_size=Config.batch_size,\n        shuffle=True,\n        num_workers=Config.num_workers,\n        pin_memory=True)\n    \n    val_dataloader = val_data_transform(model_type, data_fraction, \n                                        batch_size=Config.batch_size,\n                                        shuffle=True,\n                                        num_workers=Config.num_workers,\n                                        pin_memory=True)\n\n    \n    # Create model\n    model = SwinFaceDetector(model_name=Config.model_name,num_classes= 2)\n    model = model.to(Config.device)\n    \n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=Config.learning_rate,\n        weight_decay=Config.weight_decay\n    )\n    \n    # Learning rate scheduler\n    scheduler = None\n    if Config.scheduler_type == \"cosine\":\n        scheduler = CosineAnnealingLR(\n            optimizer,\n            T_max=Config.num_epochs,\n            eta_min=Config.min_lr\n        )\n    elif Config.scheduler_type == \"plateau\":\n        scheduler = ReduceLROnPlateau(\n            optimizer,\n            mode='min',\n            factor=Config.lr_factor,\n            patience=Config.patience,\n            verbose=True,\n            cooldown=Config.cooldown,\n            min_lr=Config.min_lr\n        )\n    \n    # Check if a saved model exists\n    if os.path.exists(Config.checkpoint_path):\n        print(f\"Loading checkpoint from {Config.checkpoint_path}\")\n        model.load_state_dict(torch.load(Config.checkpoint_path, map_location=Config.device))\n        # Skip training if model is already trained\n        df = None\n    else:\n        # Train model\n        model = train(\n            model=model,\n            train_loader=train_dataloader,\n            val_loader=val_dataloader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            num_epochs=Config.num_epochs,\n            device=Config.device\n        )\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:43.812724Z","iopub.execute_input":"2025-05-13T13:23:43.813005Z","iopub.status.idle":"2025-05-13T13:23:43.820840Z","shell.execute_reply.started":"2025-05-13T13:23:43.812983Z","shell.execute_reply":"2025-05-13T13:23:43.819930Z"}},"outputs":[],"execution_count":11},{"id":"63b438cf","cell_type":"markdown","source":"# Training Starts","metadata":{"id":"63b438cf","papermill":{"duration":0.012507,"end_time":"2024-04-07T05:17:10.936510","exception":false,"start_time":"2024-04-07T05:17:10.924003","status":"completed"},"tags":[]}},{"id":"9902352f-7389-4dad-834e-0ae419da5807","cell_type":"code","source":"if __name__ == \"__main__\":\n    main(data_fraction=1, model_type='swin', batch_size=Config.batch_size)\n    #without dropout and bn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f73f33c2","cell_type":"code","source":"if __name__ == \"__main__\":\n    main(data_fraction=1, model_type='swin', batch_size=Config.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:23:48.359723Z","iopub.execute_input":"2025-05-13T13:23:48.360442Z","iopub.status.idle":"2025-05-13T15:40:50.528729Z","shell.execute_reply.started":"2025-05-13T13:23:48.360411Z","shell.execute_reply":"2025-05-13T15:40:50.527678Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining samples: 100000\nValidation samples: 20000\n====================Starting==================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4395eb96c4e4562bd111aa9cb546042"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n----------\n✅ Unfroze 1 block(s) from the last stage. Trainable parameters: 13,129,507 / 87,271,099 (15.04%)\nUnfreezing the last 1 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 1.0e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/10]: 100%|██████████| 3125/3125 [12:03<00:00,  4.32it/s, loss=0.287] ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.181055 | Acc: 92.48% | F1: 0.924829\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.905277 | Acc: 94.63% | F1: 0.946171\n✅ Model saved at /kaggle/working/swin_.pth\nEpoch 2/10\n----------\n✅ Unfroze 2 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 2 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 9.8e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|██████████| 3125/3125 [12:46<00:00,  4.07it/s, loss=0.0617] ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.099810 | Acc: 96.20% | F1: 0.962040\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.499048 | Acc: 96.81% | F1: 0.968125\n✅ Model saved at /kaggle/working/swin_.pth\nEpoch 3/10\n----------\n✅ Unfroze 3 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 3 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 9.1e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.0412] ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.064268 | Acc: 97.64% | F1: 0.976370\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.321341 | Acc: 97.66% | F1: 0.976590\n✅ Model saved at /kaggle/working/swin_.pth\nEpoch 4/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 8.0e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.0157]  ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.046368 | Acc: 98.35% | F1: 0.983460\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.231839 | Acc: 98.72% | F1: 0.987249\n✅ Model saved at /kaggle/working/swin_.pth\nEpoch 5/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 6.6e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.024]   ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.034772 | Acc: 98.81% | F1: 0.988060\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.173859 | Acc: 97.92% | F1: 0.979241\n❌ No improvement in validation f1. Patience: 1/3\nEpoch 6/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 5.1e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.000962]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.026411 | Acc: 99.08% | F1: 0.990750\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.132055 | Acc: 99.03% | F1: 0.990299\n✅ Model saved at /kaggle/working/swin_.pth\nEpoch 7/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 3.5e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.0701]  ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.019492 | Acc: 99.31% | F1: 0.993140\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.097458 | Acc: 98.23% | F1: 0.982345\n❌ No improvement in validation f1. Patience: 1/3\nEpoch 8/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 2.1e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.0145]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.014611 | Acc: 99.49% | F1: 0.994870\nVal Loss: 0.073056 | Acc: 98.96% | F1: 0.989599\n❌ No improvement in validation f1. Patience: 2/3\nEpoch 9/10\n----------\n✅ Unfroze 4 block(s) from the last stage. Trainable parameters: 25,731,139 / 87,271,099 (29.48%)\nUnfreezing the last 4 blocks\n=====Lr=====\ncuda\nCurrent learning rate: 1.0e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/10]: 100%|██████████| 3125/3125 [12:47<00:00,  4.07it/s, loss=0.0197]  ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.011739 | Acc: 99.61% | F1: 0.996050\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.058697 | Acc: 98.96% | F1: 0.989599\n❌ No improvement in validation f1. Patience: 3/3\n⛔ Early stopping triggered.\n","output_type":"stream"}],"execution_count":12},{"id":"42a275a5","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"458b80d6-5e47-48fb-9869-e55e66ffc13d","cell_type":"markdown","source":"# Testing","metadata":{}},{"id":"0699ce5b","cell_type":"code","source":"test_dataloader = test_data_transform('swin', 1, 32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"45ddf2e9","cell_type":"code","source":"NUM_CLASSES = 2  # Update to match your model\n# Create model\nmodel = SwinFaceDetector(model_name=Config.model_name)\n\n\n# Load best model for evaluation\nmodel.load_state_dict(torch.load(Config.checkpoint_path), map_location=Config.device)\n\nmodel.to(Config.device)\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Evaluate model on test set\nprint(\"\\nEvaluating on test set:\")\ncm, report, y_true, y_pred = evaluate_model(model, test_dataloader, criterion, Config.device)\nprint(\"Classification Report:\")\nprint(report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"248262d3","cell_type":"code","source":"\n# Inference function\ndef predict_image(model, image_path, transform, device):\n    model.eval()\n    \n    image = Image.open(image_path).convert('RGB')\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        output = model(image_tensor)\n        _, pred = torch.max(output, 1)\n    \n    result = \"Fake\" if pred.item() == 0 else \"Real\"\n    prob = torch.softmax(output, dim=1)[0]\n    \n    return result, prob.cpu().numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"285cd1d5","cell_type":"markdown","source":"# Plotting\n","metadata":{}},{"id":"9f305f46","cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\ndef plot_auc_roc(y_true, y_pred, save_dir=None):\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(6, 5))\n    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}', color='blue')\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.tight_layout()\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        plot_path = os.path.join(save_dir, 'aucroc_plot.png')\n        plt.savefig(plot_path, dpi = 500)\n        print(f\"Plot saved to {plot_path}\")\n    plt.show()\n\nplot_auc_roc(y_true, y_pred, save_dir = \"/kaggle/working/output\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3ac187f8","cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n\ndef plot_acc_graphs(df, save_dir=None):\n    fig, ax = plt.subplots(figsize=(8, 5))\n\n    ax.plot(df['epoch'], df['train_accuracy'], marker='o', label='Train Accuracy')\n    ax.plot(df['epoch'], df['val_accuracy'], marker='o', linestyle='--', label='Validation Accuracy')\n\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Accuracy')\n    ax.set_title('Training vs Validation Accuracy')\n    ax.legend()\n    ax.set_ylim(bottom=0.0, top=1.0)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        plot_path = os.path.join(save_dir, 'acc_plot.png')\n        plt.savefig(plot_path, dpi=500)\n        print(f\"Plot saved to {plot_path}\")\n\n    plt.show()\n\n\ndef plot_loss_graphs(df, save_dir=None):\n    fig, ax = plt.subplots(figsize=(8, 5))\n\n    ax.plot(df['epoch'], df['train_loss'], marker='o', label='Train Loss')\n    ax.plot(df['epoch'], df['val_loss'], marker='o', linestyle='--', label='Validation Loss')\n\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('Loss')\n    ax.set_title('Training vs Validation Loss')\n    ax.legend()\n    ax.set_ylim(bottom=0.0)\n    plt.grid(True)\n    plt.tight_layout()\n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        plot_path = os.path.join(save_dir, 'loss_plot.png')\n        plt.savefig(plot_path, dpi=500)\n        print(f\"Plot saved to {plot_path}\")\n\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dbf4173a","cell_type":"code","source":"plot_acc_graphs(df, save_dir = \"/kaggle/working/output\")\nplot_loss_graphs(df, save_dir = \"/kaggle/working/output\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7d69c917","cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score, f1_score, roc_auc_score, \n    log_loss, brier_score_loss, cohen_kappa_score\n)\nimport numpy as np\n\ndef evaluate_metrics(y_true, y_pred_probs, threshold=0.5):\n    y_pred_probs = np.array(y_pred_probs)\n    y_true = np.array(y_true)\n\n    y_pred = (y_pred_probs >= threshold).astype(int)\n\n    metrics = {\n        'Accuracy': accuracy_score(y_true, y_pred),\n        'F1 Score': f1_score(y_true, y_pred),\n        'ROC AUC': roc_auc_score(y_true, y_pred_probs),\n        'Log Loss': log_loss(y_true, y_pred_probs),\n        'Brier Score': brier_score_loss(y_true, y_pred_probs),\n        'Cohen\\'s Kappa': cohen_kappa_score(y_true, y_pred),\n    }\n\n    for k, v in metrics.items():\n        print(f\"{k}: {v:.4f}\")\n\n    return metrics\n\nevaluate_metrics(y_true, y_pred, threshold=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7153b6fc","cell_type":"code","source":"from sklearn.calibration import calibration_curve\n\ndef plot_reliability_diagram(y_true, y_pred_probs, n_bins=10, save_dir=None):\n    # Compute the calibration curve\n    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred_probs, n_bins=n_bins)\n\n    # Plot the reliability diagram\n    plt.figure(figsize=(6, 5))\n    plt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Model', color='blue')\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n    plt.xlabel('Mean Predicted Probability')\n    plt.ylabel('Fraction of Positives')\n    plt.title('Reliability Diagram of FaceViT')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.tight_layout()\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        plot_path = os.path.join(save_dir, 'reliable_plot.png')\n        plt.savefig(plot_path, dpi = 500)\n        print(f\"Plot saved to {plot_path}\")\n    plt.show()\nplot_reliability_diagram(y_true, y_pred, save_dir = \"/kaggle/working/output\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4d58b9f0","cell_type":"markdown","source":"# Grad Cam AND Lime","metadata":{}},{"id":"a48bbeb7","cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef generate_gradcam(model, input_tensor, class_idx=None):\n    model.eval()\n    model.zero_grad()\n\n    # Forward pass\n    output = model(input_tensor)\n\n    # If no class is specified, use the class with highest score\n    if class_idx is None:\n        class_idx = output.argmax(dim=1).item()\n\n    # Backward pass\n    score = output[:, class_idx]\n    score.backward(retain_graph=True)\n\n    # Extract gradients and activations\n    gradients = model.gradients[0].detach().cpu().numpy()\n    activations = model.activations[0].detach().cpu().numpy()\n\n    weights = np.mean(gradients, axis=(1, 2))  # [C]\n    cam = np.zeros(activations.shape[1:], dtype=np.float32)  # [H, W]\n\n    for i, w in enumerate(weights):\n        cam += w * activations[i]\n\n    cam = np.maximum(cam, 0)\n    cam = cv2.resize(cam, (input_tensor.shape[3], input_tensor.shape[2]))\n    cam -= cam.min()\n    cam /= cam.max() + 1e-8\n\n    return cam\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3a569755","cell_type":"code","source":"def show_and_save_gradcam(img_tensor, cam, save_path=None, alpha=0.5, show=True):\n    img = img_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # unnormalize\n    img = np.clip(img, 0, 1)\n\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap = heatmap[..., ::-1] / 255.0\n\n    overlay = heatmap * alpha + img * (1 - alpha)\n    overlay = np.clip(overlay, 0, 1)\n\n    if show:\n        plt.imshow(overlay)\n        plt.axis('off')\n        plt.title(\"Grad-CAM\")\n        plt.show()\n\n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        overlay_bgr = (overlay[..., ::-1] * 255).astype(np.uint8)  # RGB to BGR\n        cv2.imwrite(save_path, overlay_bgr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ce9ef5c0","cell_type":"code","source":"def run_batch_gradcam(model, input_batch, save_dir=None, class_indices=None):\n    for i, img_tensor in enumerate(input_batch):\n        img_tensor = img_tensor.unsqueeze(0)  # Add batch dim\n        class_idx = None if class_indices is None else class_indices[i]\n\n        cam = generate_gradcam(model, img_tensor, class_idx)\n\n        save_path = f\"{save_dir}/gradcam_{i}.jpg\" if save_dir else None\n        show_and_save_gradcam(img_tensor, cam, save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"63c29c6c","cell_type":"code","source":"# For a single image:\ncam = generate_gradcam(model, input_tensor)\nshow_and_save_gradcam(input_tensor, cam, save_path='outputs/gradcam_0.jpg')\n\n# For a batch:\nrun_batch_gradcam(model, input_batch, save_dir='outputs/')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d76f4773","cell_type":"code","source":"# LIME implementation for image explanation\ndef explain_with_lime(model, image_tensor, transform, num_samples=1000):\n    \"\"\"\n    Generate LIME explanation for the model's prediction\n    \n    Args:\n        model: The trained model\n        image_tensor: Input image tensor\n        transform: The transforms applied to the input image\n        num_samples: Number of samples for LIME\n    \n    Returns:\n        Original image and explanation visualization\n    \"\"\"\n    model.eval()\n    \n    # Convert tensor to numpy image\n    img = image_tensor[0].permute(1, 2, 0).cpu().numpy()\n    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n    img = np.clip(img, 0, 1)\n    \n    # Define prediction function for LIME\n    def predict_fn(images):\n        batch = torch.stack([transform(Image.fromarray((image * 255).astype(np.uint8))) \n                            for image in images])\n        batch = batch.to(Config.device)\n        \n        with torch.no_grad():\n            preds = model(batch)\n            probs = torch.nn.functional.softmax(preds, dim=1)\n        \n        return probs.cpu().numpy()\n    \n    # Initialize LIME image explainer\n    explainer = lime_image.LimeImageExplainer()\n    \n    # Get explanation\n    explanation = explainer.explain_instance(\n        img,\n        predict_fn,\n        top_labels=2,\n        hide_color=0,\n        num_samples=num_samples\n    )\n    \n    # Get predicted class\n    pred_class = predict_fn(np.array([img]))[0].argmax()\n    \n    # Get explanation for the predicted class\n    temp, mask = explanation.get_image_and_mask(\n        pred_class, \n        positive_only=True, \n        num_features=5, \n        hide_rest=False\n    )\n    \n    # Visualization\n    explanation_img = mark_boundaries(temp, mask)\n    \n    return img, explanation_img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"28227424","cell_type":"code","source":"\n# Visualize with Grad-CAM and LIME\nprint(\"\\nGenerating model explanation visualizations...\")\n\n# Get a test image\ntest_img_path = os.path.join(Config.data_dir, \"test\", \"fake\", os.listdir(os.path.join(Config.data_dir, \"test\", \"fake\"))[0])\ntest_img = Image.open(test_img_path).convert('RGB')\ntest_img_tensor = val_transform(test_img).unsqueeze(0).to(Config.device)\n\n# Predict class\nwith torch.no_grad():\n    output = model(test_img_tensor)\n    _, pred = torch.max(output, 1)\n    pred_class = pred.item()\n    pred_label = \"Fake\" if pred_class == 1 else \"Real\"\n\nprint(f\"Generating explanations for image predicted as: {pred_label}\")\n\n# Generate Grad-CAM visualization\norig_img, grad_cam_img = generate_gradcam(model, test_img_tensor, target_class=pred_class)\n\n# Generate LIME explanation\n_, lime_img = explain_with_lime(model, test_img_tensor, val_transform)\n\n# Plot results\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(orig_img)\nplt.title(f\"Original Image\\nPredicted: {pred_label}\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(grad_cam_img)\nplt.title(\"Grad-CAM Visualization\")\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.imshow(lime_img)\nplt.title(\"LIME Explanation\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.savefig('model_explanations.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}